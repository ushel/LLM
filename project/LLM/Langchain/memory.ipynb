{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fd792f",
   "metadata": {},
   "source": [
    "Naturally models dont remember what you say or any of previous conversation.\n",
    "\n",
    "Its an issue when building application such as chatbot.\n",
    "\n",
    "Memory come in pictures its basically remember previous parts of the conversation and feed that into the language model to get conversation flow \n",
    "\n",
    "such as you are interacting with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e36b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1943f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 7\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4525f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ef3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi my name is Utkarsh\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Utkarsh! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation.predict(input=\"Hi my name is Utkarsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed01e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73fdceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Utkarsh.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54efb31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Utkarsh.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf76f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi my name is Utkarsh\\nAI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Utkarsh.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba1f49",
   "metadata": {},
   "source": [
    "How to explicity store in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69c692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e697bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76335bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818ab53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcf0ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Not much just hanging\"},{\"output\":\"cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da7e3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much just hanging\n",
      "AI: cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b638d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much just hanging\\nAI: cool\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ee5b0",
   "metadata": {},
   "source": [
    "When you use LLM for a chat conversation, LLM is actually stateless.\n",
    "\n",
    "The Language model itself does not remember the conversation you had so far.\n",
    "\n",
    "Each Transaction, each call to api endpoint is independent.\n",
    "\n",
    "Chatbots appear to have memory only because there is usually  rapid code that provides the full conversation as context to LLM\n",
    "\n",
    "This memory storage can be used as additional context or input to LLM models to generate output.\n",
    "\n",
    "As conversation becomes long, the storage increases also the number of tokens send increases increasing the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6e8f3",
   "metadata": {},
   "source": [
    "Lets look at different type of memory, ConversationBufferWindowMemory keeps windows of memory with parameter k\n",
    "\n",
    "K = 1 means want to save one conversation exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4636a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "790faa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_7251/3475767457.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory =ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "memory =ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd4d2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"What's up\"})\n",
    "memory.save_context({\"input\":\"Not much, just hanging\"},{\"output\":\"cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49abd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb1702d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: cool'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1103d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation=ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5fc9c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Utkarsh\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Utkarsh! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Utkarsh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b358be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: what is 1+1\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86f0add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: what is 1+1\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c50fb",
   "metadata": {},
   "source": [
    "ConversationTokenBufferMemory :- The memory will limit to the number of tokens saved. Which will map directly to the call to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3aa04ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0899cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_7251/1059215224.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit = 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59926122",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"AI is what?!\"},{\"output\":\"Amazing\"})\n",
    "memory.save_context({\"input\":\"Backpropagation is what?!\"},{\"output\":\"Beautiful!\"})\n",
    "memory.save_context({\"input\":\"Chatbots are what?\"},{\"output\":\"Charming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1b55017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing\\nHuman: Backpropagation is what?!\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "860997fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Amazing\n",
      "Human: Backpropagation is what?!\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb448a",
   "metadata": {},
   "source": [
    "Here we can see it has chopped off 1st input sentence from memory as token limit is 50 so. to get full sentences we will have to increase token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5889ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationTokenBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit =100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f46ec95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1.save_context({\"input\":\"AI is what?!\"},{\"output\":\"Amazing\"})\n",
    "memory1.save_context({\"input\":\"Backpropagation is what?!\"},{\"output\":\"Beautiful!\"})\n",
    "memory1.save_context({\"input\":\"Chatbots are what?\"},{\"output\":\"Charming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc714b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: AI is what?!\n",
      "AI: Amazing\n",
      "Human: Backpropagation is what?!\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming\n"
     ]
    }
   ],
   "source": [
    "print(memory1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f298e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing\\nHuman: Backpropagation is what?!\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0b975",
   "metadata": {},
   "source": [
    "we can see here all the inputs and outputs are store in the memory as we have increased max_token_limit size to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2cc3c",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory :- Here it stores the conversation it had soo far and is summarize and store in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2b7ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"There is the meeting at 8am with your product team \\\n",
    "    you will need to provide powerpoint presentation prepared. \\\n",
    "    9am -12pm have a time to work on your langchain \\\n",
    "    project which will go quickly because langchain is such a powerful tool\\\n",
    "    At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "    from over an hour away to meet you to understand the latest in AI.\\\n",
    "    Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dac346a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_7251/3508865618.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8956597",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hello\"},{\"output\":\"What's up\"})\n",
    "memory.save_context({\"input\":\"Not much, just hanging\"},{\"output\":\"cool\"})\n",
    "memory.save_context({\"input\":\"What is on the schedule today?\"},{\"output\":f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de4c953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: cool\n",
      "Human: What is on the schedule today?\n",
      "AI: There is the meeting at 8am with your product team     you will need to provide powerpoint presentation prepared.     9am -12pm have a time to work on your langchain     project which will go quickly because langchain is such a powerful tool    At Noon, lunch at the italian resturant with a customer who is driving     from over an hour away to meet you to understand the latest in AI.    Be sure to bring your laptop to show the latest LLM demo.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2c37ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: cool\\nHuman: What is on the schedule today?\\nAI: There is the meeting at 8am with your product team     you will need to provide powerpoint presentation prepared.     9am -12pm have a time to work on your langchain     project which will go quickly because langchain is such a powerful tool    At Noon, lunch at the italian resturant with a customer who is driving     from over an hour away to meet you to understand the latest in AI.    Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d849b0",
   "metadata": {},
   "source": [
    "here we have set max_token_limit to 400. so we are getting whole output.\n",
    "\n",
    "what happens if we decrease max_token_limit to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b59df1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit = 100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3f53e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1.save_context({\"input\":\"Hello\"},{\"output\":\"What's up\"})\n",
    "memory1.save_context({\"input\":\"Not much, just hanging\"},{\"output\":\"cool\"})\n",
    "memory1.save_context({\"input\":\"What is on the schedule today?\"},{\"output\":f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6e92b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human greets the AI and asks about the schedule for the day. The AI informs the human of a meeting at 8am with the product team, work on the langchain project from 9am-12pm, and a lunch meeting with a customer at noon to discuss the latest in AI. The AI emphasizes the importance of being prepared for the day's activities.\n"
     ]
    }
   ],
   "source": [
    "print(memory1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "689b3ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI and asks about the schedule for the day. The AI informs the human of a meeting at 8am with the product team, work on the langchain project from 9am-12pm, and a lunch meeting with a customer at noon to discuss the latest in AI. The AI emphasizes the importance of being prepared for the day's activities.\"}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf33748",
   "metadata": {},
   "source": [
    "Here we can see it has used LLM to summarize the conversation and limiting token limits to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30459890",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory1,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22006141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks about the schedule for the day. The AI informs the human of a meeting at 8am with the product team, work on the langchain project from 9am-12pm, and a lunch meeting with a customer at noon to discuss the latest in AI. The AI emphasizes the importance of being prepared for the day's activities.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"For the meeting with the product team, a demo showcasing the latest features and updates on the langchain project would be ideal. This could include a live demonstration of the new language processing capabilities or a comparison of performance metrics with previous versions. Additionally, for the lunch meeting with the customer, a demo highlighting the AI's ability to personalize recommendations or improve user experience could be impactful. It's important to tailor the demos to the specific audience and their interests to make a strong impression.\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db28175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
