{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fd792f",
   "metadata": {},
   "source": [
    "Naturally models dont remember what you say or any of previous conversation.\n",
    "\n",
    "Its an issue when building application such as chatbot.\n",
    "\n",
    "Memory come in pictures its basically remember previous parts of the conversation and feed that into the language model to get conversation flow \n",
    "\n",
    "such as you are interacting with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e36b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1943f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 7\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4525f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ef3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi my name is Utkarsh\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Utkarsh! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation.predict(input=\"Hi my name is Utkarsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed01e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73fdceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Utkarsh.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54efb31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Utkarsh.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf76f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi my name is Utkarsh\\nAI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Utkarsh.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba1f49",
   "metadata": {},
   "source": [
    "How to explicity store in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69c692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e697bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76335bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818ab53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcf0ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Not much just hanging\"},{\"output\":\"cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da7e3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much just hanging\n",
      "AI: cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b638d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much just hanging\\nAI: cool\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ee5b0",
   "metadata": {},
   "source": [
    "When you use LLM for a chat conversation, LLM is actually stateless.\n",
    "\n",
    "The Language model itself does not remember the conversation you had so far.\n",
    "\n",
    "Each Transaction, each call to api endpoint is independent.\n",
    "\n",
    "Chatbots appear to have memory only because there is usually  rapid code that provides the full conversation as context to LLM\n",
    "\n",
    "This memory storage can be used as additional context or input to LLM models to generate output.\n",
    "\n",
    "As conversation becomes long, the storage increases also the number of tokens send increases increasing the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6e8f3",
   "metadata": {},
   "source": [
    "Lets look at different type of memory, ConversationBufferWindowMemory keeps windows of memory with parameter k\n",
    "\n",
    "K = 1 means want to save one conversation exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4636a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "790faa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_7251/3475767457.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory =ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "memory =ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd4d2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"What's up\"})\n",
    "memory.save_context({\"input\":\"Not much, just hanging\"},{\"output\":\"cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49abd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb1702d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: cool'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1103d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation=ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5fc9c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Utkarsh\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Utkarsh! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Utkarsh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b358be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Utkarsh\n",
      "AI: Hello Utkarsh! It's nice to meet you. How can I assist you today?\n",
      "Human: what is 1+1\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86f0add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: what is 1+1\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c50fb",
   "metadata": {},
   "source": [
    "ConversationTokenBufferMemory :- The memory will limit to the number of tokens saved. Which will map directly to the call to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3aa04ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0899cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_7251/1059215224.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit = 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59926122",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"AI is what?!\"},{\"output\":\"Amazing\"})\n",
    "memory.save_context({\"input\":\"Backpropagation is what?!\"},{\"output\":\"Beautiful!\"})\n",
    "memory.save_context({\"input\":\"Chatbots are what?\"},{\"output\":\"Charming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1b55017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing\\nHuman: Backpropagation is what?!\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "860997fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Amazing\n",
      "Human: Backpropagation is what?!\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb448a",
   "metadata": {},
   "source": [
    "Here we can see it has chopped off 1st input sentence from memory as token limit is 50 so. to get full sentences we will have to increase token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5889ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationTokenBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit =100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f46ec95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1.save_context({\"input\":\"AI is what?!\"},{\"output\":\"Amazing\"})\n",
    "memory1.save_context({\"input\":\"Backpropagation is what?!\"},{\"output\":\"Beautiful!\"})\n",
    "memory1.save_context({\"input\":\"Chatbots are what?\"},{\"output\":\"Charming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc714b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: AI is what?!\n",
      "AI: Amazing\n",
      "Human: Backpropagation is what?!\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming\n"
     ]
    }
   ],
   "source": [
    "print(memory1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f298e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing\\nHuman: Backpropagation is what?!\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0b975",
   "metadata": {},
   "source": [
    "we can see here all the inputs and outputs are store in the memory as we have increased max_token_limit size to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7ee2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
