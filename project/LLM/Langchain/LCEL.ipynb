{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1b8151",
   "metadata": {},
   "source": [
    "Lnagchain composes chains of components\n",
    "\n",
    "ICEL and the runnable protocol define:\n",
    "\n",
    "1. An allowed set of input types \n",
    "\n",
    "2. An allowed set of output types\n",
    "\n",
    "3. Standard methods (invoke, stream, batch) - Means of modifying parameters at runtime (bind)\n",
    "\n",
    "syntax looks like\n",
    "\n",
    "chain = prompt | llm | OutputParser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ffa28f",
   "metadata": {},
   "source": [
    "Interface :- \n",
    "\n",
    "1. Components implement Runnable protocol\n",
    "\n",
    "2. Common methods includes:- \n",
    "    \n",
    "    1. Invoke -synchronus method [ainvoke] -asynchronus\n",
    "    \n",
    "    2. stream -synchronus method [astream] -asynchronus\n",
    "\n",
    "    3. batch -synchronus method [abatch] -asynchronus\n",
    "\n",
    "3. Common properties:- \n",
    "\n",
    "    input_schema, output_schema\n",
    "\n",
    "4. Common I/O\n",
    "\n",
    "Component           |Input Type                                 |Output Type|\n",
    "|:--:|:--:     |:--:|\n",
    "Prompt              |Dictionary                                 |Prompt Value\n",
    "|:--:|:--:     |:--:|\n",
    "Retriever           |Single String                              |List of Documents\n",
    "|:--:|:--:     |:--:|\n",
    "LLM                 |String, List of messages or prompt value   |String\n",
    "|:--:|:--:     |:--:|\n",
    "ChatModel           |String, List of messages or prompt value   |ChatMessage\n",
    "|:--:|:--:     |:--:|\n",
    "Tool                |String/Dictionary                          |Tool Dependent\n",
    "|:--:|:--:     |:--:|\n",
    "OutputParser        |Output or LLM or ChatModel                 |Parser dependent\n",
    "|:--:|:--:     |:--:|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8480d6",
   "metadata": {},
   "source": [
    "Why use LCEL :- \n",
    "\n",
    "Runnables Support:-\n",
    "\n",
    "1. Async, Batch and Streaming support\n",
    "\n",
    "2. Fallbacks\n",
    "\n",
    "3. Parallelism\n",
    "    1. LLM calls can be time consuming\n",
    "    2. Any components that can be run in parallel are!\n",
    "\n",
    "4. Logging is built in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904712e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6a910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db89385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
